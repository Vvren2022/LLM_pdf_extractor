{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = grisha_key"
      ],
      "metadata": {
        "id": "5Az8RG6DVGHL"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain.llms import  OpenAI"
      ],
      "metadata": {
        "id": "DgBTIPPiVbxH"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install langchain-community"
      ],
      "metadata": {
        "id": "NvQ_IrSqVs03"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5-2ZVrHVzyV",
        "outputId": "11bcf263-f469-4ecb-b6ad-9fa144031302"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.22)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.49 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.49)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.7 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.7)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.22)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain) (9.0.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain) (4.13.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.49->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "prompt_template = PromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
        "\n",
        "prompt_template.invoke({\"topic\": \"cats\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PR_osFIBbiF_",
        "outputId": "e07c3463-1c57-4524-b166-d8dd94d86a6d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StringPromptValue(text='Tell me a joke about cats')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt_template = ChatPromptTemplate([\n",
        "    (\"system\", \"You are a helpful assistant\"),\n",
        "    (\"user\", \"Tell me a joke about {topic}\")\n",
        "])\n",
        "\n",
        "response=prompt_template.invoke({\"topic\": \"cats\"})\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4dsup-McIdW",
        "outputId": "bc6362ae-82c0-43b8-c88e-917ded59c2c9"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me a joke about cats', additional_kwargs={}, response_metadata={})])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "prompt_template = ChatPromptTemplate([\n",
        "    (\"system\", \"You are a helpful assistant\"),\n",
        "    MessagesPlaceholder(\"msgs\")\n",
        "])\n",
        "\n",
        "prompt_template.invoke({\"msgs\": [HumanMessage(content=\"hi!\")]})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RiekCo3cP61",
        "outputId": "e3f330e2-907b-46b3-ccd8-8602a01547d2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant', additional_kwargs={}, response_metadata={}), HumanMessage(content='hi!', additional_kwargs={}, response_metadata={})])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install PyPDF2\n",
        "# !pip install tiktoken"
      ],
      "metadata": {
        "id": "RJ16OiVOclyv"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PyPDF2 import PdfReader\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS"
      ],
      "metadata": {
        "id": "-xUBKYyXJ353"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -U langchain-community\n",
        "# !pip install fitz"
      ],
      "metadata": {
        "id": "vgITSIYdL-UA"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PyPDF2 import PdfReader\n",
        "\n",
        "reader = PdfReader('/content/vren_vaviya_final_resume.pdf')\n",
        "\n",
        "\n",
        "\n",
        "import pdfplumber\n",
        "\n",
        "raw_text = \"\"  # Initialize an empty string to store the extracted text\n",
        "\n",
        "with pdfplumber.open(\"/content/vren_vaviya_final_resume.pdf\") as pdf:\n",
        "    for page in pdf.pages:\n",
        "        text = page.extract_text()\n",
        "        if text:  # Check if text is extracted\n",
        "            raw_text += text   # Append text with spacing\n",
        "\n",
        "raw_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "3Wz2Doh0P4M6",
        "outputId": "97fad650-3334-41d3-dc39-8338b0cb6727"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n",
            "WARNING:pdfminer.pdfpage:CropBox missing from /Page, defaulting to MediaBox\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"VIREN VAVIYA\\n\\uf879Phone number: (+49) 015560047336\\n\\uf0e0Email address: vrenvaviya2022@gmail.com\\n\\uf086 github: https://github.com/Vvren2022\\n\\uf08c LinkedIn: linkedin.com/in/viren-vaviya-a9933423b/\\n\\uf3c5 Home: Frankfurt am Main (Germany)\\nABOUT ME\\nSelf-motivated and detail-oriented Data Analyst with a strong foundation in data visualization, statistical analysis, and\\nSQL. Passionate about leveraging AI and machine learning to transform raw data into actionable insights that drive\\nstrategic decision-making. Proficient in Python, Power BI, and modern AI frameworks, with hands-on experience in\\nworking with large datasets. Continuously exploring emerging technologies, including Generative AI and LLMs, to\\nenhance analytical capabilities and develop innovative solutions. A fast learner with a problem-solving mindset,\\nalways eager to push the boundaries of data science.\\nWORK EXPERIENCE\\nData scientist | Dev's Club\\n[ 10/06/2023 – 11/06/2024 ]\\nCity: Surat | Country: India\\n• Advanced Python Programming: Expertise in Python for data science, leveraging Pandas, NumPy, Scipy, and\\nDask for efficient data preprocessing, feature engineering, and scalable analytics.\\n• Data Visualization & Business Intelligence: Proficient in Matplotlib, Seaborn, Plotly, and Power BI/Tableau\\nto create interactive dashboards and visualize complex datasets for data-driven decision-making.\\n• Machine Learning & Deep Learning: Hands-on experience in Supervised and Unsupervised Learning, Neural\\nNetworks, CNNs, RNNs, LSTMs, GRUs, and Transfer Learning using TensorFlow, Keras, PyTorch, and Scikit-\\nlearn. Experience in model deployment with MLflow, FastAPI, Flask, and Docker.\\n• Natural Language Processing (NLP) & Large Language Models (LLMs): Experience with text preprocessing,\\ntokenization, embeddings (Word2Vec, BERT, OpenAI GPT), Named Entity Recognition (NER), and Sentiment\\nAnalysis. Skilled in LangChain, Hugging Face Transformers, and vector database integration (FAISS, Chromadb).\\n• Time Series Analysis & Forecasting: Expertise in ARIMA, SARIMA, Prophet, Exponential Smoothing, and\\nLSTMs for forecasting. Applied seasonal decomposition, stationarity tests (ADF, KPSS), and hypothesis testing\\nto improve predictive performance.\\n• Generative AI & Automation: Worked on AI-driven automation using OpenAI APIs, LangChain, AutoML, and\\nReinforcement Learning for intelligent decision-making systems.\\n• SQL & Databases: Strong skills in SQL for data extraction, manipulation, and performance optimization.\\nPROJECTS\\n[ 01/01/2023 – 05/03/2023 ]\\nStock Market Price Prediction | Dev's Club\\n•Built deep learning models (LSTM, GRU, CNN) using TensorFlow/Keras for stock price forecasting.\\n•Conducted ARIMA, SARIMA, and Prophet-based statistical modeling to analyze seasonal trends and market\\nbehavior.\\n•Engineered features using SQL & Pandas, handling large-scale financial datasets.\\n•Integrated Power BI for interactive financial dashboards, providing predictive analytics for investment\\nstrategies.\\nLink:https://github.com/Vvren2022/stoke_price_predictionGrishaBot: AI-Powered News Research Tool\\n•Developed an AI-driven research assistant using OpenAI embeddings, LangChain, and FAISS for real-time\\nsemantic search.\\n•Implemented automated web scraping using LangChain's WebpageReader, BeautifulSoup, and Scrapy,\\nextracting and preprocessing text data with NLTK, spaCy, and Hugging Face.\\n•Designed an interactive Streamlit UI, integrating LLMs (GPT, PaLM) and Retrieval-Augmented Generation (RAG)\\nfor accurate responses.Optimized document retrieval and knowledge management using vector databases\\n(FAISS, ChromaDB) and SQL for efficient data processing.\\nLink:https://github.com/Vvren2022/research_tools_LLM\\n[ 12/07/2024 – 07/10/2024 ]\\nGrisha_Q: Intelligent FAQ-Based Question Answering System |personal project\\n•Developed an AI-powered FAQ system using LangChain, OpenAI,Google PaLM, Hugging Face embeddings, and\\nFAISS for efficient similarity search.\\n•Transformed static FAQ data into a dynamic knowledge base with vector databases for real-time answer\\nretrieval.\\n•Applied RetrievalQA techniques with OpenAI and Google’s models for accurate, conversational query\\nresolution.\\n•Built a scalable system to automate customer interactions, enhancing user engagement and reducing manual\\neffort.\\n•Delivered a seamless chatbot experience, improving efficiency and reducing the need for manual intervention.\\nLink:https://github.com/Vvren2022/grisha_Q_-_A\\nPROGRAMMING LANGUAGES\\nPython | SQL\\nTECHNOLOGY & TOOLS\\nStatsmodels | MySQL | POWER BI | Scikit-learn | TensorFlow | Keras | Matplotlib | Seaborn | Pandas |\\nNumPy\\nSKILLS\\nMachine Learning | NLP | Data Preprocessing & Manipulation | LLM(Generative AI)\\nLANGUAGE SKILLS\\nMother tongue(s): gujarati\\nOther language(s): German | English\\nEDUCATION AND TRAINING\\nMaster's in High Integrity Systems\\nFrankfurt University of Applied Sciences [ 10/04/2024 – Current ]\\nCity: Frankfurt am Main | Country: Germany\\nBachelor of Engineering (B.E.) – Information and Technology\\nGEC Gandhinagar [ 19/06/2019 – 16/06/2023 ]\\nCity: gandhinagar | Country: India\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install pdfplumber\n"
      ],
      "metadata": {
        "id": "rXiuaYLjSO7y"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H9LMdlrbPBtG"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Iz9Bct-xPB1W"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We need to split the text using Character Text Split such that it sshould not increse token size\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    separator = \"\\n\",\n",
        "    chunk_size = 800,\n",
        "    chunk_overlap  = 200,\n",
        "    length_function = len,\n",
        ")\n",
        "texts = text_splitter.split_text(raw_text)"
      ],
      "metadata": {
        "id": "MJwunG6nM3TL"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# texts"
      ],
      "metadata": {
        "id": "cfkuSaLLM9Rk"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wi6VTW-M_hj",
        "outputId": "bdf08ce7-4978-4f89-8c26-fc71f65d1c91"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = OpenAIEmbeddings()"
      ],
      "metadata": {
        "id": "lL7KbYvANP0X"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document_search = FAISS.from_texts(texts, embeddings)"
      ],
      "metadata": {
        "id": "BN0DAe3lNVCW"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install faiss-cpu"
      ],
      "metadata": {
        "id": "BscYvLLDNhU2"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.llms import OpenAI"
      ],
      "metadata": {
        "id": "lTADKD-GNlbw"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = load_qa_chain(OpenAI(), chain_type=\"stuff\")"
      ],
      "metadata": {
        "id": "hv03KANxNtjR"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"experince fields\"\n",
        "docs = document_search.similarity_search(query)\n",
        "chain.run(input_documents=docs, question=query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "U4U4FErWNwtR",
        "outputId": "8d7a443c-73a3-4fcc-9450-2f6b7a1bd0f4"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' The experience fields listed in the given context are: Machine Learning & Deep Learning, Natural Language Processing (NLP) & Large Language Models (LLMs), Time Series Analysis & Forecasting, Generative AI & Automation, SQL & Databases.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"How much the agriculture target will be increased to and what the focus will be\"\n",
        "docs = document_search.similarity_search(query)\n",
        "chain.run(input_documents=docs, question=query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "mdngsHK6N70H",
        "outputId": "e5ee7d66-b52f-4062-8579-d173181078e8"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' The agriculture credit target will be increased to `20 lakh crore with a focus on animal husbandry, dairy, and fisheries.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z0h3VHGgOeNE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}